{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL\n",
    "\n",
    "## training 正常 val/testing 壞掉\n",
    "* 檢查dropout, batchnorm 是否正常設置\n",
    "* 檢查 momentum/decay 設置\n",
    "\n",
    "## BN momentum/decay （各地名稱不同）\n",
    "關係到 running mean 的更新速度  \n",
    "(running mean * momentum) + (batch mean * (1-momentum))  \n",
    "pytorch 的 momentum 倒過來 (設0.1 = 保留0.9)  \n",
    "\n",
    "若 momentum 太大可能導致 val/testing 時 loss 飆高/亂跳\n",
    "\n",
    "### default value\n",
    "* tensorflow\n",
    "    * tf.layer 0.99\n",
    "    * tf.contrib 0.999\n",
    "    * tf.contrib.slim 0.999\n",
    "* keras 0.99\n",
    "* pytorch 0.1 (實際等同 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 運行效能\n",
    "\n",
    "## 跑太慢了啊啊啊啊\n",
    "* 檢查 nvidia-smi\n",
    "    * GPU 上有多個 process\n",
    "        * 要跟別人分算力，所以慢\n",
    "    * 算力沒用滿\n",
    "        * feed input 的速度不夠快\n",
    "            * multi thread(IO)\n",
    "            * multi process(CPU)\n",
    "        * 太常存 model\n",
    "            * 存的間距拉大點\n",
    "\n",
    "## 還想更快\n",
    "* data_format 改用 channel_first(cuDNN)\n",
    "* BN 的 fused 設為 True\n",
    "* 用 Dataset API 取代 feed dict(multi GPU 較明顯)\n",
    "\n",
    "## multi  GPU\n",
    "* 做法\n",
    "    * 同步\n",
    "        * average grad or loss\n",
    "    * 非同步\n",
    "        * 分別 update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# pytorch\n",
    "\n",
    "## load DataParallel model weight\n",
    "直接改名稱(笑\n",
    "\n",
    "```python\n",
    "from collections import OrderedDict\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in w.items():\n",
    "    name = k[7:]\n",
    "    new_state_dict[name] = v\n",
    "```\n",
    "\n",
    "## batchnorm 在 testing 壞掉\n",
    "* 記得切換為 eval mode \n",
    "```python\n",
    "model.eval()\n",
    "```\n",
    "\n",
    "## single GPU 正常, multi GPU testing 壞掉\n",
    "可能是 batch 中的資料分佈不均（如前半都是 OK, 後半都是 NG）  \n",
    "因為 BN 是 GPU 各做各的,會導致最後保留的 moving mean 偏向某種 class  \n",
    "解決辦法: batch 內也要做 shuffle\n",
    "\n",
    "## tensors are on different GPUs\n",
    "* 檢查 model/tensor 是否都是 GPU mode\n",
    "* 如有 custom function, 確認資料維持在同個 GPU 上\n",
    "```python\n",
    "orig_tensor.data.new(new_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensorflow\n",
    "\n",
    "## batchnorm 在 testing 壞掉\n",
    "可能沒有更新 moving_mean  \n",
    "keras 有設 set_learning_phase 也還是要做\n",
    "\n",
    "方案一\n",
    "```python\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_op = optimizer.minimize(loss)\n",
    "```\n",
    "\n",
    "方案二 (only for contrib/slim )\n",
    "```python\n",
    "tf.contrib.layers.batch_norm(x, updates_collections=None) \n",
    "```\n",
    "\n",
    "## get variable list\n",
    "```python\n",
    "# all variables\n",
    "tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "\n",
    "# all trainable variables\n",
    "tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "tf.trainable_variables()\n",
    "\n",
    "# not trainable but should be update (such as moving mean in BN layer)\n",
    "tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "```\n",
    "\n",
    "## select parameters to optimize\n",
    "```python\n",
    "opt.minimize(loss, var_list=[v for tf.trainable_variables() in var_list if \"CONV\" in v.name])\n",
    "```\n",
    "\n",
    "## restore part of variables\n",
    "```python\n",
    "saver = tf.train.Saver([v for v in var_list if 'CONV' in v.name and 'biases' not in v.name])\n",
    "saver.restore(sess, 'tf_ckpt/resnet_v2_50.ckpt')\n",
    "```\n",
    "\n",
    "## GAP\n",
    "```python\n",
    "# GAP 2D (NHWC)\n",
    "x = tf.reduce_mean(x, [1,2])\n",
    "```\n",
    "\n",
    "## change variable names in ckpt\n",
    "```python\n",
    "scope = 'CONV_1'\n",
    "ckpt = 'resnet_v1_50'\n",
    "\n",
    "# get var list (name, shape)\n",
    "vars = tf.contrib.framework.list_variables('{}.ckpt'.format(ckpt))\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session().as_default() as sess:\n",
    "    new_vars = []\n",
    "    for name, shape in vars:\n",
    "        # get weight from ckpt by name\n",
    "        v = tf.contrib.framework.load_variable('{}.ckpt'.format(ckpt), name)\n",
    "        new_vars.append(tf.Variable(v, name='{}/{}'.format(scope, name)))\n",
    "\n",
    "    saver = tf.train.Saver(new_vars)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.save(sess, './{}_{}.ckpt'.format(ckpt, scope))\n",
    "```\n",
    "\n",
    "## load keras pretrained model\n",
    "```python\n",
    "model1 = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_tensor=input_img)\n",
    "conv_output = model1.get_layer('activation_49').output\n",
    "```\n",
    "\n",
    "### You must feed a value for placeholder tensor '.../.../keras_learning_phase' with dtype bool\n",
    "```python\n",
    "is_training = main_graph.get_tensor_by_name('keras_learning_phase:0')\n",
    "```\n",
    "\n",
    "## Attempting to use uninitialized value\n",
    "```python\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "```\n",
    "\n",
    "## multi GPU\n",
    "calculate grad in different GPUS and apply mean grad  \n",
    "```python\n",
    "tower_grads = []\n",
    "\n",
    "with tf.variable_scope(tf.get_variable_scope()):\n",
    "    for i in range(n_gpus):\n",
    "        with tf.device('/gpu:{}'.format(i)):\n",
    "            with tf.name_scope('TOWER_{}'.format(i)) as scope:\n",
    "                x, y = reader.dequeue(batch_size)\n",
    "                loss = tower_loss(x, y)\n",
    "\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "                grads = opt.compute_gradients(loss)\n",
    "                tower_grads.append(grads)\n",
    "                \n",
    "grads = average_gradients(tower_grads)\n",
    "update = opt.apply_gradients(grads)\n",
    "```\n",
    "\n",
    "## channel last/first\n",
    "預設為 channel last, 可由 data_format 設定  \n",
    "一般圖片開起來是 channel last, 可用 img.transpose((2, 0, 1)) 交換  \n",
    "\n",
    "channel first 會比較快（測試起來約快一成？）  \n",
    "因為符合 cuDNN 的 order  \n",
    "可參考 https://www.tensorflow.org/performance/performance_guide#data_formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF pretrained model\n",
    "\n",
    "小心各種不同的 preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow-slim\n",
    "https://github.com/tensorflow/models/tree/master/research/slim\n",
    "\n",
    "* Inception 1~4 & Inception-ResNet-v2\n",
    "* ResNet v1/v2 50~200\n",
    "* VGG 16/19\n",
    "* MobileNet\n",
    "* NASNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 坑\n",
    "* preprocess 都不一樣\n",
    "* 記得確認 BN 的 decay\n",
    "* 最好用自帶的 argscope\n",
    "* 改成 NCHW 如果報錯，可能是因為 dimension, padding 等等是寫死的，要自己改 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keras\n",
    "\n",
    "* Xception\n",
    "* VGG16\n",
    "* VGG19\n",
    "* ResNet50\n",
    "* InceptionV3\n",
    "* InceptionResNetV2\n",
    "* MobileNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://github.com/flyyufelix/DenseNet-Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DenseNet  121~169"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
